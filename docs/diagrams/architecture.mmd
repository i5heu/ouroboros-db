classDiagram
    class Cluster {
        -[]Node Nodes
    }

    class Node {
        +string NodeID
        +[]string Addresses
        +Cert NodeCert
    }

    class ClusterController {
    }

    class ClusterMonitor {
        +MonitorNodeHealth()
        +CollectClusterLogs()
        +GetDataState()
        +CollectNodeStats()
        +GetNodeStats(nodeID string) (NodeStats, error)
    }

    %% ClusterLog: Cluster-wide log aggregation.
    %% Purpose:
    %% - Single place to collect operational logs/events from every Node.
    %% - Makes it easy to debug replication, routing, and failures without SSH-ing into nodes.
    %% - Enables cluster-level timelines ("what happened across the system at time T?").
    %%
    %% What it contains:
    %% - Timestamped LogEntry records tagged with NodeID + Level and optional structured fields.
    %%
    %% What it is NOT:
    %% - Not the long-term analytics system; it is a conceptual component for centralized visibility.
    class ClusterLog {
        +Append(entry LogEntry)
        +Tail(limit int) []LogEntry
        +Query(nodeID string, since int64) []LogEntry
        +QueryAll(since int64) []LogEntry
        +QueryByLevel(level LogLevel, since int64) []LogEntry
    }

    class LogEntry {
        +int64 Timestamp
        +string NodeID
        +LogLevel Level
        +string Message
        +map~string, string~ Fields
    }

    class LogLevel {
        <<Enumeration>>
        Debug
        Info
        Warn
        Error
    }

    %% DataState: Cluster-wide data placement + status view.
    %% Purpose:
    %% - Answer "which node has which data" and "what is its current state" at a glance.
    %% - Provides an operational index for replication monitoring, rebalancing, repair, and offline sync.
    %%
    %% Scope of "Data": the objects modeled in Logical_Hierarchy + Block_Storage_Modern:
    %% - Vertex (logical DAG nodes)
    %% - SealedChunk (encrypted chunk payloads)
    %% - KeyEntry (access-control entries)
    %% - Block and BlockSlice (physical archive and shards)
    %%
    %% Status examples:
    %% - Present/Missing (inventory)
    %% - Syncing (in-progress replication)
    %% - PendingDelete (DeletionWAL/GC lifecycle)
    %% - Corrupt (integrity mismatch / failed reconstruction)
    class DataState {
        +GetNodesForVertex(vertexHash hash.Hash) ([]NodeDataStatus, error)
        +GetNodesForSealedChunk(chunkHash hash.Hash) ([]NodeDataStatus, error)
        +GetNodesForBlock(blockHash hash.Hash) ([]NodeDataStatus, error)
        +GetNodesForBlockSlice(sliceHash hash.Hash) ([]NodeDataStatus, error)
        +GetNodeInventory(nodeID string) ([]NodeDataStatus, error)
    }

    class NodeDataStatus {
        +string NodeID
        +hash.Hash DataHash
        +DataStatus Status
        +string Detail
    }

    class DataStatus {
        <<Enumeration>>
        Present
        Missing
        Syncing
        PendingDelete
        Corrupt
    }

    %% NodeStats: Per-node storage statistics (fast "at a glance" view).
    %% Purpose:
    %% - Quick capacity/health signal: how much the node is storing across major object types.
    %% - Supports dashboards and placement decisions (e.g., rebalancer choosing less-loaded nodes).
    %% - Helps identify skew (too many slices/blocks on one node) and replication gaps.
    %%
    %% Notes:
    %% - Counts cover the same "Data" scope as DataState (Logical_Hierarchy + Block_Storage_Modern).
    %% - "Chunks" at rest are represented as SealedChunks (encrypted payloads).
    class NodeStats {
        +string NodeID
        +int64 Updated
        +uint64 VertexCount
        +uint64 BlockCount
        +uint64 BlockSliceCount
        +uint64 SealedChunkCount
        +uint64 KeyEntryCount
    }

    class NodeAvailabilityTracker {
        +TrackAvailability()
    }

    class Carrier {
        +GetNodes() []Node
        +Broadcast(message Message) (success []Node, error)
        +SendMessageToNode(nodeID NodeID, message Message) error
        
        +JoinCluster(clusterNode Node, Cert NodeCert) error
        +LeaveCluster(clusterNode Node) error
    }

    class BootStrapper {
        +BootstrapNode(node Node) error
    }

    Carrier "1" *-- "1" BootStrapper : initializes

    class Message  {
        +MessageType Type
        +[]byte Payload
    }
    Carrier "1" o-- "*" Message : sends/receives

    class MessageType {
        <<Enumeration>>
        BlockSliceRequest
        BlockSliceResponse
        ChunkMetaRequest
        VertexMetaRequest
        %% Heartbeat is also used for monitoring information
        Heartbeat
        NodeJoinRequest
        NodeLeaveNotification
        UserAuthDecision
        NewNodeAnnouncement
        KeyEntryRequest
        KeyEntryResponse
        BlockSyncRequest
    }
    Message "1" o-- "1" MessageType : has type


    %% DataRouter: Routes data operations across the cluster.
    %% Coordinates with CAS for content operations and handles
    %% distribution of blocks/slices to appropriate nodes.
    class DataRouter {
        +StoreVertex(vertex Vertex) (hash.Hash, error)
        +RetrieveVertex(hash.Hash) (Vertex, error)
        +DeleteVertex(hash.Hash) error
        +DistributeBlockSlices(block Block) error
        +RetrieveBlock(hash.Hash) (Block, error)
    }

    %% BlockDistributionTracker: Tracks block distribution and confirmations.
    %% Responsible for tracking slice confirmations, pending distributions,
    %% and providing metadata for offline node sync.
    class BlockDistributionTracker {
        +StartDistribution(block Block, walKeys [][]byte) (*BlockDistributionRecord, error)
        +RecordSliceConfirmation(blockHash hash.Hash, sliceHash hash.Hash, nodeID string) (bool, error)
        +GetDistributionState(blockHash hash.Hash) (*BlockDistributionRecord, error)
        +GetPendingDistributions() ([]*BlockDistributionRecord, error)
        +MarkDistributed(blockHash hash.Hash) error
        +MarkFailed(blockHash hash.Hash, reason string) error
        +GetDistributedBlocksSince(since int64) ([]hash.Hash, error)
        +GetBlockMetadata(blockHash hash.Hash) (*BlockMetadata, error)
    }

    %% CAS: Content Addressable Storage - the main management layer.
    %% Provides high-level API for content operations, coordinating
    %% encryption, WAL buffering, block storage, and access control.
    class CAS {
        %% Content operations (high-level API)
        +StoreContent(content []byte, parentHash hash.Hash) (Vertex, error)
        +GetContent(vertexHash hash.Hash) ([]byte, error)
        +DeleteContent(vertexHash hash.Hash) error
        
        %% Vertex operations
        +GetVertex(hash.Hash) (Vertex, error)
        +ListChildren(parentHash hash.Hash) ([]Vertex, error)
        
        %% Internal dependencies
        -DistributedWAL wal
        -BlockStore blockStore
        -EncryptionService encryptor
    }

    %% BlockStore: Low-level Block and BlockSlice persistence.
    %% Handles physical storage and retrieval of blocks and their shards.
    class BlockStore {
        +StoreBlock(Block) error
        +GetBlock(hash.Hash) (Block, error)
        +DeleteBlock(hash.Hash) error
        +StoreBlockSlice(BlockSlice) error
        +GetBlockSlice(hash.Hash) (BlockSlice, error)
        +ListBlockSlices(blockHash hash.Hash) ([]BlockSlice, error)

        %% Region-based retrieval for efficient partial reads
        +GetSealedChunkByRegion(blockHash hash.Hash, region ChunkRegion) (SealedChunk, error)
        +GetVertexByRegion(blockHash hash.Hash, region VertexRegion) (Vertex, error)
    }

    %% EncryptionService: Handles encryption/decryption of chunks.
    %% Manages the Chunk <-> SealedChunk transformation.
    class EncryptionService {
        +SealChunk(Chunk, pubKeys [][]byte) (SealedChunk, []KeyEntry, error)
        +UnsealChunk(SealedChunk, KeyEntry, privKey []byte) (Chunk, error)
        +GenerateKeyEntry(chunkHash hash.Hash, pubKey []byte, aesKey []byte) (KeyEntry, error)
    }

    class DeletionWAL {
        +LogDeletion(hash.Hash) error
        +ProcessDeletions() error
    }

    class DistributedIndex 

    class HashToNode {
        +GetNodeForHash(hash.Hash) Node
    }

    class KeyToHashAndNode {
        +GetHashAndNodeForKey(string) (hash.Hash, Node, error)
    }

    class DataReBalancer {
        +BalanceData()
    }

    class ReplicationMonitoring {
        +MonitorReplications()
    }

    class SyncIndexTree {
        +Sync()
    }

    class BackupManager {
        +BackupData()
    }

    Cluster "1" *-- "*" Node : contains
    Node "1" o-- "*" ClusterController : listensOn
    ClusterController "1" *-- "1" Carrier : communicatesVia
    Node "1" o-- "1" CAS : manages content
    Node "1" *-- "1" BlockStore : persists blocks
    DataRouter "1" *-- "1" ClusterController : interacts with
    CAS "1" *-- "1" DataRouter : coordinates distribution via
    DataRouter "1" o-- "1" BlockDistributionTracker : initiates distribution via
    ClusterController "1" o-- "1" BlockDistributionTracker : queries for offline node sync
    BlockDistributionTracker "1" *-- "*" Block : tracks distribution for
    CAS "1" *-- "1" BlockStore : stores blocks via
    CAS "1" *-- "1" EncryptionService : encrypts/decrypts via
    CAS "1" *-- "1" DistributedWAL : buffers writes via
    ClusterController "1" *-- "1" DistributedIndex : LooksUps
    DistributedIndex "1" *-- "1" HashToNode : used for mapping
    DistributedIndex "1" *-- "1" KeyToHashAndNode : lookups for keys
    ClusterController "1" *-- "1" DataReBalancer : manages
    DataReBalancer "1" *-- "1" ReplicationMonitoring : utilizes
    ClusterController "1" *-- "1" ClusterMonitor : monitors
    Node "1" *-- "1" BackupManager : manages backups
    ClusterMonitor "1" *-- "1" NodeAvailabilityTracker : utilizes
    ClusterMonitor "1" *-- "1" ClusterLog : aggregates
    ClusterMonitor "1" *-- "1" DataState : tracks
    ClusterMonitor "1" o-- "*" NodeStats : collects
    DataReBalancer "1" *-- "1" SyncIndexTree : utilizes
    Node "1" *-- "1" DeletionWAL : logs deletions

    %% Nodes emit logs into the cluster-wide log.
    Node ..> ClusterLog : emits logs to

    %% DataState maps data to nodes with status.
    %% Tracks objects from Logical_Hierarchy + Block_Storage_Modern:
    %% Vertex, SealedChunk, Block, BlockSlice, KeyEntry.
    DataState "1" o-- "*" Node : maps data to nodes

    %% NodeStats are associated with nodes.
    Node "1" o-- "*" NodeStats : reports

    %% (Diagram simplification) LogEntry/LogLevel are internal to ClusterLog.


    namespace IndexModel {
        class Index {
            -LocalIndexStore store
        }
        class parentChildIndex {
            - map<hash.Hash, []hash.Hash> ParentToChildren
            - map<hash.Hash, hash.Hash> ChildToParent
        }
        class VersionIndex {
            - map<hash.Hash, []hash.Hash> VersionVectorHeads
        }
        class KeyToHashIndex {
            - map<string, hash.Hash> KeyToHash
        }
    }

    Node "1" o-- "1" Index : indexes relations and metadata
    Index "1" o-- "1" parentChildIndex : manages
    Index "1" o-- "1" VersionIndex : manages

    namespace Logical_Hierarchy {
        %% Vertex (formerly Blob): The logical node in the DAG.
        %% stored UNENCRYPTED in the VertexSection of the Block.
        class Vertex {
            +hash.Hash Hash
            +hash.Hash Parent
            +int64 Created
            -[]hash.Hash ChunkHashes
            +GetContent() []byte
        }

        %% Chunk: The cleartext content.
        %% Only exists temporarily in memory during processing/decryption.
        class Chunk {
            +hash.Hash Hash
            +int Size
            -[]byte content
            +GetContent() []byte
        }
    }

    namespace Block_Storage_Modern {
        %% DistributedWAL: Intake buffer.
        %% Aggregates items until Block size (e.g., 16MB) is reached.
        class DistributedWAL {
            +AppendChunk(SealedChunk)
            +AppendVertex(Vertex)
            +SealBlock() Block
            -[]SealedChunk chunkBuffer
            -[]Vertex vertexBuffer
        }

        %% SealedChunk: The encrypted payload.
        %% Now includes SealedHash for integrity checking of the encrypted data 
        %% without needing decryption keys.
        class SealedChunk {
            +hash.Hash ChunkHash
            +hash.Hash SealedHash
            +[]byte EncryptedContent
            +[]byte Nonce
            +int OriginalSize
        }

        %% KeyEntry: The Access Control unit.
        %% Explicitly links the User (PubKey) to the Content (ChunkHash).
        class KeyEntry {
            +hash.Hash ChunkHash
            +hash.Hash PubKeyHash
            +[]byte EncapsulatedAESKey
        }

        %% The Indices that allow for byte range like requests
        %% This prevents that we need to decode the whole Block to get a single Chunk/Vertex.
        class ChunkRegion {
            +hash.Hash ChunkHash
            +uint32 Offset
            +uint32 Length
        }
        class VertexRegion {
            +hash.Hash VertexHash
            +uint32 Offset
            +uint32 Length
        }

        %% BlockHeader: Global block settings.
        class BlockHeader {
            +uint8 Version
            +int64 Created
            +uint8 RSDataSlices
            +uint8 RSParitySlices
            +uint32 ChunkCount
            +uint32 VertexCount
            +uint32 TotalSize
        }

        %% Block: The Central Archive.
        class Block {
            +hash.Hash Hash
            +BlockHeader Header
            +[]byte DataSection
            +[]byte VertexSection
            +map~Hash, []KeyEntry~ KeyRegistry
        }

        %% BlockSlice: The physical shard.
        class BlockSlice {
            +hash.Hash Hash
            +hash.Hash BlockHash
            +uint8 RSSliceIndex
            +uint8 RSDataSlices
            +uint8 RSParitySlices
            +[]byte Payload
        }
    }

    %% ==========================================
    %% 1. LOGICAL RELATIONSHIPS
    %% ==========================================
    Vertex "1" ..> "*" Chunk : references (via ChunkHash)

    %% ==========================================
    %% 2. PIPELINE & TRANSFORMATION
    %% ==========================================
    Chunk ..> SealedChunk : encrypts to
    SealedChunk --* DistributedWAL : buffered in
    Vertex --* DistributedWAL : buffered in
    DistributedWAL ..> Block : produces (SealBlock)

    %% ==========================================
    %% 3. BLOCK COMPOSITION
    %% ==========================================
    %% Structure
    Block "1" *-- "1" BlockHeader : has
    Block "1" *-- "*" ChunkRegion : indexes
    Block "1" *-- "*" VertexRegion : indexes
    
    %% Payloads
    Block "1" *-- "*" SealedChunk : contains (in DataSection)
    Block "1" *-- "*" Vertex : contains (in VertexSection)
    
    %% Registries
    Block "1" *-- "*" KeyEntry : registry (Access Control)

    %% ==========================================
    %% 4. PHYSICAL STORAGE
    %% ==========================================
    BlockSlice ..> Block : shards/reconstructs
    BlockStore "1" o-- "*" Block : persists
    BlockStore "1" o-- "*" BlockSlice : persists
    
    %% ==========================================
    %% 5. FUNCTIONAL LINKS
    %% ==========================================
    ChunkRegion ..> SealedChunk : locates bytes of
    VertexRegion ..> Vertex : locates bytes of
    KeyEntry ..> SealedChunk : unlocks (via ChunkHash)
    EncryptionService ..> SealedChunk : produces
    EncryptionService ..> KeyEntry : produces