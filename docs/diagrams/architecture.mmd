classDiagram
    class Cluster {
        -[]Node Nodes
    }

    class Node {
        +NodeID NodeID
        +[]string Addresses
        +NodeCert NodeCert
    }

    %% Security profile (strict):
    %% - Node identity signatures MUST use ML-DSA-87.
    %% - Session key exchange MUST use X25519Kyber768.
    %% - Transport MUST negotiate PQ-hybrid TLS suites only.
    %% - Any downgrade/fallback to weaker profile MUST fail closed.
    %% NodeCert: Node identity certificate payload.
    %% Contents:
    %% - Carries the node public key material (composite keys.PublicKey: KEM + Sign).
    %% - Identifies the issuer CA by hash (IssuerCAHash).
    %%   - There can be multiple AdminCAs and multiple UserCAs.
    %%   - IssuerCAHash allows selecting the expected CA without trial-verifying many roots.
    %% - Includes signed lifecycle/authorization metadata:
    %%   CertVersion, ValidFrom, ValidUntil, Serial, RoleClaims, CertNonce.
    %% NodeID derivation:
    %% - NodeID is NOT a free-form name.
    %% - NodeID is derived ONLY from the node public key material:
    %%   NodeID = SHA-256(NodePubKey).
    %% - AdminCA/UserCA signatures authorize the node but do not change its NodeID.
    %% Signature scope (what is signed):
    %% - The CA signature MUST be over a canonical serialization of the ENTIRE NodeCert
    %%   structure (including IssuerCAHash and any future metadata like expiry/constraints).
    %% - Sign/Verify MUST use cryptographic domain separation to prevent type confusion
    %%   between different signed payload types.
    %% - Required form:
    %%   CA_Signature = Sign(CTX_NODE_ADMISSION_V1 || CanonicalSerialize(NodeCert))
    %% - This follows the "what you see is what was signed" rule and prevents mutable fields.
    %% Identity model:
    %% - There is NO name/CN/SAN-based identity. The only stable identity is NodeID derived
    %%   from NodePubKey().
    %% - Any presented NodeID must be treated as informational unless recomputed from NodePubKey().
    %% Security (proof of possession / binding):
    %% - The peer is ONLY authenticated if transport proof includes an explicit delegation
    %%   signed by NodePubKey().publicSign:
    %%   DelegationSig = Sign(CTX_NODE_DELEGATION_V1 || CanonicalSerialize(DelegationProof)).
    %% - DelegationProof MUST bind TLSCertPubKeyHash + TLSExporterBinding +
    %%   X509Fingerprint + NodeCertHash.
    %% - Replay/UKS defense requires short delegation TTL, single-use handshake nonce, and
    %%   identity binding to the TLS transcript.
    class NodeCert {
        +NodePubKey() keys.PublicKey
        +IssuerCAHash() CaHash.Hash
        +ValidFrom() time.Time
        +ValidUntil() time.Time
        +Serial() []byte
        +RoleClaims() []string
        +CertNonce() []byte
        +NodeID() keys.NodeID
    }

    %% DelegationProof: Session delegation envelope signed by Node identity key.
    %% Must be tightly bound to the active transport handshake.
    class DelegationProof {
        +TLSCertPubKeyHash() []byte
        +TLSExporterBinding() []byte
        +X509Fingerprint() []byte
        +NodeCertHash() []byte
        +NotBefore() time.Time
        +NotAfter() time.Time
        +HandshakeNonce() []byte
    }

    %% TrustScope: Authentication scope of a connected peer.
    %% Hierarchy: ScopeAdmin implies ScopeUser (Admin has all User permissions).
    %%
    %% ScopeAdmin: AdminCA-authorized node.
    %% - May perform system/cluster actions.
    %% - May request any data (subject to cluster policy), not tied to a specific user.
    %% - Can invoke handlers registered for ScopeAdmin OR ScopeUser.
    %%
    %% ScopeUser: UserCA-authorized node.
    %% - Must be sandboxed; never allow system/cluster actions.
    %% - May request ONLY data belonging to the user identity represented by the issuing
    %%   UserCA (peerCert.IssuerCAHash()) while the node is connected and not revoked.
    %% - No per-request user signature is required because the node identity is already
    %%   authenticated (PoP) and authorized (CA admission); the authorization boundary is
    %%   enforced by scoping all requests to that issuing UserCAHash.
    %% - Can only invoke handlers registered for ScopeUser (unless also allowed ScopeAdmin).
    %% - Role claims MUST resolve to exactly one of ScopeAdmin or ScopeUser;
    %%   missing/invalid/ambiguous claims MUST fail authentication.
    %%
    %% Handler Registration:
    %% - Handlers specify []TrustScope (peer needs ANY of these scopes - OR logic).
    %% - Example: []TrustScope{ScopeAdmin, ScopeUser} allows both Admin and User peers.
    class TrustScope {
        <<Enumeration>>
        ScopeAdmin
        ScopeUser
    }

    %% ClusterController: Central control layer for node cluster operations.
    %% Handler Management (HTTP-style message handlers):
    %% - RegisterHandler: Associates a MessageType with handler function and allowed TrustScopes
    %% - Handlers execute with peer authentication context (NodeID + TrustScope)
    %% - No default handler: unregistered message types are rejected
    %%
    %% TrustScope Hierarchy & Access Control:
    %% - ScopeAdmin implies ScopeUser (Admin peers can access User handlers)
    %% - Handlers specify []TrustScope (OR logic - peer needs ANY allowed scope)
    %% - Example: RegisterHandler(msgType, []TrustScope{ScopeAdmin, ScopeUser}, handler)
    %%   allows both Admin and User peers to invoke the handler
    %%
    %% Authorization Flow:
    %% 1. Carrier receives message and authenticates peer
    %% 2. Carrier delivers (Message, NodeID, TrustScope) to ClusterController
    %% 3. ClusterController looks up handler by MessageType
    %% 4. CheckAccess() validates if peer's effective scopes match handler's allowed scopes
    %% 5. If authorized: HandleIncomingMessage() invokes handler and returns Response
    %% 6. If denied: returns error with reason
    %%
    %% Effective Scope Calculation:
    %% - GetEffectiveScopes(ScopeAdmin) returns [ScopeAdmin, ScopeUser]
    %% - GetEffectiveScopes(ScopeUser) returns [ScopeUser]
    class ClusterController {
        %% Handler registration
        +RegisterHandler(msgType MessageType, scopes []TrustScope, handler MessageHandler) error
        +UnregisterHandler(msgType MessageType) error
        +GetHandler(msgType MessageType) (HandlerRegistration, bool)
        
        %% Message processing
        +HandleIncomingMessage(msg Message, peer NodeID, peerScope TrustScope) (Response, error)
        +CheckAccess(msgType MessageType, peerScope TrustScope) AccessDecision
        +GetEffectiveScopes(scope TrustScope) []TrustScope
    }

    %% MessageHandler: HTTP-style handler function signature.
    %% Context carries request lifecycle and cancellation.
    %% Message contains the deserialized request payload.
    %% Peer identifies the authenticated remote node.
    %% Scope indicates the authenticated trust level of the peer.
    %% Returns Response with payload and optional error.
    class MessageHandler {
        <<function>>
        func(ctx context.Context, msg Message, peer NodeID, scope TrustScope) (Response, error)
    }

    %% HandlerRegistration: Stores a registered handler with its access requirements.
    %% MessageType: The message type this handler processes.
    %% AllowedScopes: Peer must have ANY of these scopes (OR logic).
    %% Handler: The actual handler function to execute.
    class HandlerRegistration {
        +MessageType MessageType
        +[]TrustScope AllowedScopes
        +MessageHandler Handler
    }

    %% Response: Handler response envelope returned to caller.
    %% Payload: Serialized response data.
    %% Error: Processing error (if any).
    %% Metadata: Optional key-value pairs for response context.
    class Response {
        +[]byte Payload
        +error Error
        +map~string, string~ Metadata
    }

    %% AccessDecision: Result of authorization check.
    %% Allowed: true if peer may invoke the handler.
    %% Reason: Human-readable explanation if denied.
    class AccessDecision {
        +bool Allowed
        +string Reason
    }

    class ClusterMonitor {
        +MonitorNodeHealth()
        +CollectClusterLogs()
        +GetDataState()
        +CollectNodeStats()
        +GetNodeStats(nodeID string) (NodeStats, error)
    }

    %% ClusterLog: Cluster-wide log aggregation.
    %% Purpose:
    %% - Single place to collect operational logs/events from every Node.
    %% - Makes it easy to debug replication, routing, and failures without SSH-ing into nodes.
    %% - Enables cluster-level timelines ("what happened across the system at time T?").
    %%
    %% What it contains:
    %% - Timestamped LogEntry records tagged with NodeID + Level and optional structured fields.
    %%
    %% What it is NOT:
    %% - Not the long-term analytics system; it is a conceptual component for centralized visibility.
    class ClusterLog {
        +New(logger *slog.Logger, carrier Carrier, selfID NodeID) *ClusterLog
        +Stop()
        +Log(ctx context.Context, level LogLevel, msg string, fields map~string, string~)
        +Info(ctx context.Context, msg string, fields map~string, string~)
        +Warn(ctx context.Context, msg string, fields map~string, string~)
        +Debug(ctx context.Context, msg string, fields map~string, string~)
        +Err(ctx context.Context, msg string, fields map~string, string~)
        +Tail(limit int) []LogEntry
        +Query(nodeID NodeID, since time.Time) []LogEntry
        +QueryAll(since time.Time) []LogEntry
        +QueryByLevel(level LogLevel, since time.Time) []LogEntry
        +SubscribeLog(ctx context.Context, sourceNodeID NodeID, subscriberNodeID NodeID)
        +SubscribeLogAll(ctx context.Context, subscriberNodeID NodeID)
        +UnsubscribeLog(ctx context.Context, sourceNodeID NodeID, subscriberNodeID NodeID)
        +UnsubscribeLogAll(ctx context.Context, subscriberNodeID NodeID)
        +SendLog(ctx context.Context, targetNodeID NodeID, sourceNodeID NodeID, since time.Time) error
    }

    class LogEntry {
        +time.Time Timestamp
        +keys.NodeID NodeID
        +LogLevel Level
        +string Message
        +map~string, string~ Fields
    }

    class LogLevel {
        <<Enumeration>>
        Debug
        Info
        Warn
        Error
    }

    %% DataState: Cluster-wide data placement + status view.
    %% Purpose:
    %% - Answer "which node has which data" and "what is its current state" at a glance.
    %% - Provides an operational index for replication monitoring, rebalancing, repair, and offline sync.
    %%
    %% Scope of "Data": the objects modeled in Logical_Hierarchy + Block_Storage_Modern:
    %% - Vertex (logical DAG nodes)
    %% - SealedChunk (encrypted chunk payloads)
    %% - KeyEntry (access-control entries)
    %% - Block and BlockSlice (physical archive and shards)
    %%
    %% Status examples:
    %% - Present/Missing (inventory)
    %% - Syncing (in-progress replication)
    %% - PendingDelete (DeletionWAL/GC lifecycle)
    %% - Corrupt (integrity mismatch / failed reconstruction)
    class DataState {
        +GetNodesForVertex(vertexHash hash.Hash) ([]NodeDataStatus, error)
        +GetNodesForSealedChunk(chunkHash hash.Hash) ([]NodeDataStatus, error)
        +GetNodesForBlock(blockHash hash.Hash) ([]NodeDataStatus, error)
        +GetNodesForBlockSlice(sliceHash hash.Hash) ([]NodeDataStatus, error)
        +GetNodeInventory(nodeID string) ([]NodeDataStatus, error)
    }

    class NodeDataStatus {
        +string NodeID
        +hash.Hash DataHash
        +DataStatus Status
        +string Detail
    }

    class DataStatus {
        <<Enumeration>>
        Present
        Missing
        Syncing
        PendingDelete
        Corrupt
    }

    %% NodeStats: Per-node storage statistics (fast "at a glance" view).
    %% Purpose:
    %% - Quick capacity/health signal: how much the node is storing across major object types.
    %% - Supports dashboards and placement decisions (e.g., rebalancer choosing less-loaded nodes).
    %% - Helps identify skew (too many slices/blocks on one node) and replication gaps.
    %%
    %% Notes:
    %% - Counts cover the same "Data" scope as DataState (Logical_Hierarchy + Block_Storage_Modern).
    %% - "Chunks" at rest are represented as SealedChunks (encrypted payloads).
    class NodeStats {
        +string NodeID
        +int64 Updated
        +uint64 VertexCount
        +uint64 BlockCount
        +uint64 BlockSliceCount
        +uint64 SealedChunkCount
        +uint64 KeyEntryCount
    }

    class NodeAvailabilityTracker {
        +TrackAvailability()
    }

    %% Carrier: QUIC-based cluster transport with dual-mode delivery.
    %%
    %% Transport:
    %% - QUIC (RFC 9000) with multiplexing
    %% - RFC 9221 Unreliable Datagrams for UDP-style delivery
    %% - Standard QUIC streams for TCP-style reliable delivery
    %%
    %% Connections:
    %% - Persistent connections to ALL known nodes
    %% - Immediate disconnect on node removal
    %%
    %% Bootstrap:
    %% - Static bootstrap list in BootstrapConfig
    %% - Used for initial cluster join
    %%
    %% Node Sharing:
    %% - Periodic full sync via NodeSync
    %% - All nodes share all admin/user-signed nodes
    %% - Full NodeCert + CA signatures exchanged
    %%
    %% Trust:
    %% - Authentication via pkg/auth/carrier_auth.go
    %% - Dynamic CA/node add/remove supported
    class Carrier {
        +GetNodes() []Node
        +GetNode(nodeID NodeID) (Node, error)
        +BroadcastReliable(message Message) (success []Node, failed []Node, err error)
        +SendMessageToNodeReliable(nodeID NodeID, message Message) error
        +BroadcastUnreliable(message Message) (attempted []Node)
        +SendMessageToNodeUnreliable(nodeID NodeID, message Message) error
        +Broadcast(message Message) (success []Node, err error)
        +SendMessageToNode(nodeID NodeID, message Message) error
        +JoinCluster(clusterNode Node, nodeCert NodeCert) error
        +LeaveCluster(clusterNode Node) error
        +RemoveNode(nodeID NodeID) error
        +IsConnected(nodeID NodeID) bool
    }

    %% QuicTransport: QUIC implementation with streams + datagrams.
    class QuicTransport {
        +Dial(node Node) (Connection, error)
        +Accept() (Connection, error)
        +Close() error
        +GetActiveConnections() []Connection
    }

    %% Connection: Single QUIC connection to a peer.
    class Connection {
        +NodeID NodeID
        +OpenStream() (Stream, error)
        +AcceptStream() (Stream, error)
        +SendDatagram(data []byte) error
        +ReceiveDatagram() ([]byte, error)
        +Close() error
    }

    %% Stream: Reliable QUIC stream.
    class Stream {
        +Read([]byte) (int, error)
        +Write([]byte) (int, error)
        +Close() error
    }

    %% BootstrapConfig: Static bootstrap node list.
    class BootstrapConfig {
        +[]Node BootstrapNodes
        +LoadFromFile(path string) error
        +SaveToFile(path string) error
    }

    %% NodeRegistry: Tracks all known nodes with certificates.
    class NodeRegistry {
        +AddNode(node Node, cert NodeCert) error
        +RemoveNode(nodeID NodeID) error
        +GetNode(nodeID NodeID) (Node, error)
        +GetAllNodes() []Node
        +GetAdminNodes() []Node
        +GetUserNodes() []Node
    }

    %% NodeSync: Periodic full sync of node registry.
    class NodeSync {
        +StartSyncInterval(duration Duration)
        +StopSync()
        +TriggerFullSync() error
        +ExchangeNodeList(peer NodeID) error
        +HandleNodeListUpdate(nodes []NodeInfo) error
    }

    %% NodeInfo: Complete node data for sync.
    class NodeInfo {
        +Node Node
        +NodeCert NodeCert
        +CASignature []byte
        +LastSeen time.Time
        +ConnectionStatus ConnectionStatus
    }

    %% ConnectionStatus: Node connection state.
    class ConnectionStatus {
        <<Enumeration>>
        Connected
        Disconnected
        Connecting
        Failed
    }

    %% AdminCA: Cluster admin certificate authority (one trust root).
    %% Trust model:
    %% - The cluster may trust MULTIPLE AdminCAs (multi-root). Each AdminCA represents
    %%   exactly one Admin signing public key + its identifier.
    %% - AdminCA is verification-only: signing/issuance happens off-device/out-of-band.
    %% What it verifies:
    %% - A "node certificate" is the peer-presented NodeCert plus a detached CA signature.
    %% - VerifyNodeCert checks the CA signature is valid under this Admin pubkey over the
    %%   domain-separated payload:
    %%   SignInput = CTX_NODE_ADMISSION_V1 || CanonicalSerialize(NodeCert)
    %%   (not over a name or string identifier).
    %% - On success, the returned NodeID MUST be computed from the NodeCert itself:
    %%   NodeID = SHA-256(NodeCert.NodePubKey).
    %% - This verifies issuance/authorization of the NodeCert, NOT proof-of-possession.
    %% Proof of possession (PoP):
    %% - PoP is provided by delegated transport binding (DelegationProof + DelegationSig)
    %%   and TLS ownership of SessionPrivKey.
    %% Hash semantics:
    %% - Hash() is the SHA-256 identifier of the Admin public key.
    %% - Nodes can exchange/compare AdminCAHash values to agree on which Admin root is trusted
    %%   without transmitting full key material.
    %% Enforcement:
    %% - A peer is considered an "admin-authorized node" only if (1) VerifyNodeCert succeeds
    %%   under at least one trusted AdminCA and (2) the NodeID is not revoked by CarrierAuth.
    class AdminCA {
        +PubKey() sign.PublicKey
        +Hash() CaHash.Hash
        +VerifyNodeCert(nodeCert NodeCert, caSignature []byte) (nodeID NodeID, error)
    }

    %% UserCA: User certificate authority (one trust root).
    %% Purpose:
    %% - Represents a user identity root for user-scoped node admission.
    %% - MUST be anchored by an AdminCA signature (cryptographic chain of trust).
    %% Identification:
    %% - Hash() is the SHA-256 identifier of the UserCA public key (UserCAHash).
    %% - Requesting nodes can send only UserCAHash to reference the correct UserCA.
    %% Anchor requirement:
    %% - Every UserCA MUST have an anchor signature from a trusted AdminCA.
    %% - AnchorSig is the AdminCA's signature over CTX_USER_CA_ANCHOR_V1 || UserCAPubKey.
    %% - AnchorAdminHash identifies which AdminCA signed this UserCA.
    %% - If the anchor AdminCA is removed or revoked, the UserCA becomes untrusted.
    %% Node admission:
    %% - VerifyNodeCert validates user-signed node certificates.
    %% - The signature is always a cryptographic signature over the domain-separated NodeCert
    %%   payload (never a name):
    %%   CTX_NODE_ADMISSION_V1 || CanonicalSerialize(NodeCert).
    %% - On success, the returned NodeID MUST be computed from the NodeCert itself:
    %%   NodeID = SHA-256(NodeCert.NodePubKey).
    %% - Nodes admitted under a UserCA are "user-scoped" and must be restricted to data
    %%   belonging to that UserCA (not full cluster authority).
    %% Proof of possession (PoP):
    %% - As with AdminCA, PoP is provided by delegated transport binding and TLS ownership
    %%   of SessionPrivKey.
    class UserCA {
        +PubKey() sign.PublicKey
        +Hash() CaHash.Hash
        +AnchorSig() []byte
        +AnchorAdminHash() CaHash.Hash
        +VerifyNodeCert(nodeCert NodeCert, caSignature []byte) (nodeID NodeID, error)
    }

    %% CarrierAuth (pkg): Carrier-side authentication + trust-root management.
    %% Inputs:
    %% - Carrier extracts identity material from handshake/auth exchange:
    %%   - peerCert: NodeCert
    %%   - caSignature: detached CA signature over CTX_NODE_ADMISSION_V1 || canonical(NodeCert)
    %%   - delegationProof: includes TLSCertPubKeyHash, TLSExporterBinding,
    %%     X509Fingerprint, NodeCertHash,
    %%     NotBefore, NotAfter, HandshakeNonce
    %%   - delegationSig: signature over CTX_NODE_DELEGATION_V1 || canonical(delegationProof)
    %%   - tlsCertPubKeyHash: hash of QUIC/TLS peer leaf SubjectPublicKeyInfo
    %%   - tlsExporterBinding: exporter bytes bound to TLS transcript
    %%   - tlsX509Fingerprint: fingerprint of presented TLS cert
    %%   - tlsTranscriptHash: handshake transcript hash for binding checks
    %% Decision:
    %% - VerifyPeerCert attempts to authenticate the peer as either:
    %%   - admin-authorized node: NodeCert verifies under a trusted AdminCA, OR
    %%   - user-scoped node: NodeCert verifies under a trusted UserCA.
    %% - On success it returns the authenticated NodeID AND the TrustScope.
    %% - On failure it returns error and the connection MUST be rejected.
    %% Validation requirements (fail closed):
    %% - Unknown IssuerCAHash MUST reject.
    %% - NodeCert time window MUST be valid (ValidFrom..ValidUntil).
    %% - Node/issuer revocation status MUST be fresh (TTL-bounded), else reject.
    %% - VerifyNodeCert MUST succeed under exactly one trusted issuer path.
    %% - delegationSig MUST verify under peerCert.NodePubKey().publicSign.
    %% - delegationProof.TLSCertPubKeyHash MUST equal tlsCertPubKeyHash.
    %% - delegationProof.TLSExporterBinding MUST equal tlsExporterBinding.
    %% - delegationProof.X509Fingerprint MUST equal tlsX509Fingerprint.
    %% - delegationProof.NodeCertHash MUST equal hash(peerCert).
    %% - HandshakeNonce MUST be unused and recorded as single-use.
    %% - Effective identity MUST bind to tlsTranscriptHash to prevent UKS.
    %% Namespace collision requirement:
    %% - Because NodeID is CA-independent, callers MUST NOT use NodeID alone to grant
    %%   system privileges.
    %% - Callers MUST enforce authorization using TrustScope:
    %%   - ScopeAdmin => eligible for system-level actions and cluster-wide data requests
    %%   - ScopeUser  => sandboxed; never allow system-level actions
    %%     - may request ONLY data belonging to the issuing UserCAHash of the authenticated
    %%       NodeCert (peerCert.IssuerCAHash())
    %% Trust-root management:
    %% - Add/RemoveAdminPubKey updates the trusted AdminCA set (keyed by AdminCAHash).
    %% - Add/RemoveUserPubKey updates the trusted UserCA set (keyed by UserCAHash).
    %% Revocation:
    %% - RevokeNode denies a NodeID even if it presents a valid certificate.
    %% - Revocation applies to both admin-authorized and user-scoped nodes.
    %% - RevokeAdminCA / RevokeUserCA disables a CA by hash (stronger than Remove*; intended
    %%   to prevent re-adding the same root without an explicit un-revoke policy).
    %% Authorization boundary (important):
    %% - Authentication answers "who is the peer node?" (NodeID + which CA validated it).
    %% - Authorization answers "what may it do?" and must be enforced by higher-level
    %%   components (e.g., routing/ACL), especially for user-scoped nodes.
    class CarrierAuth {
        +VerifyPeerCert(peerCert NodeCert, caSignature []byte, delegationProof DelegationProof, delegationSig []byte, tlsCertPubKeyHash []byte, tlsExporterBinding []byte, tlsX509Fingerprint []byte, tlsTranscriptHash []byte) (nodeID NodeID, scope TrustScope, error)
        +AddAdminPubKey(pubKey sign.PublicKey) error
        +RemoveAdminPubKey(pubKeyHash CaHash.Hash) error
        +RevokeAdminCA(adminCAHash CaHash.Hash) error
        +AddUserPubKey(pubKey sign.PublicKey, anchorSig []byte, anchorAdminHash CaHash.Hash) error
        +RemoveUserPubKey(pubKeyHash CaHash.Hash) error
        +RevokeUserCA(userCAHash CaHash.Hash) error
        +RevokeNode(nodeID NodeID) error
    }

    class BootStrapper {
        +BootstrapNode(node Node) error
    }
    
    class BootstrapAuthFunc {
        <<function>>
        func(conn Connection) (nodeID NodeID, scope TrustScope, error)
    }

    Carrier "1" *-- "1" BootStrapper : initializes
    Carrier "1" *-- "1" CarrierAuth : authenticatesVia
    Carrier "1" *-- "1" QuicTransport : uses
    Carrier "1" *-- "1" NodeRegistry : manages
    Carrier "1" *-- "1" NodeSync : syncsVia
    Carrier "1" *-- "1" BootstrapConfig : configuredBy
    QuicTransport "1" o-- "*" Connection : maintains
    Connection "1" o-- "*" Stream : provides
    NodeRegistry "1" o-- "*" NodeInfo : stores
    NodeSync ..> NodeRegistry : updates
    CarrierAuth ..> "1..*" AdminCA : verifiesPeerCertWith
    CarrierAuth ..> "0..*" UserCA : verifiesPeerCertWith
    CarrierAuth ..> "1" DelegationProof : validatesBindingWith
    AdminCA ..> Node : validatesAdminSignedNodeCertFor
    UserCA ..> Node : validatesUserSignedNodeCertFor
    UserCA ..> AdminCA : anchoredBy

    class Message  {
        +MessageType Type
        +[]byte Payload
    }
    Carrier "1" o-- "*" Message : sends/receives

    class MessageType {
        <<Enumeration>>
        BlockSliceRequest
        BlockSliceResponse
        ChunkMetaRequest
        VertexMetaRequest
        %% Heartbeat is also used for monitoring information
        Heartbeat
        NodeJoinRequest
        NodeLeaveNotification
        UserAuthDecision
        NewNodeAnnouncement
        KeyEntryRequest
        KeyEntryResponse
        BlockSyncRequest
        LogPush
        LogSendResponse
    }
    Message "1" o-- "1" MessageType : has type


    %% DataRouter: Routes data operations across the cluster.
    %% Coordinates with CAS for content operations and handles
    %% distribution of blocks/slices to appropriate nodes.
    class DataRouter {
        +StoreVertex(vertex Vertex) (hash.Hash, error)
        +RetrieveVertex(hash.Hash) (Vertex, error)
        +DeleteVertex(hash.Hash) error
        +DistributeBlockSlices(block Block) error
        +RetrieveBlock(hash.Hash) (Block, error)
    }

    %% BlockDistributionTracker: Tracks block distribution and confirmations.
    %% Responsible for tracking slice confirmations, pending distributions,
    %% and providing metadata for offline node sync.
    class BlockDistributionTracker {
        +StartDistribution(block Block, walKeys [][]byte) (*BlockDistributionRecord, error)
        +RecordSliceConfirmation(blockHash hash.Hash, sliceHash hash.Hash, nodeID string) (bool, error)
        +GetDistributionState(blockHash hash.Hash) (*BlockDistributionRecord, error)
        +GetPendingDistributions() ([]*BlockDistributionRecord, error)
        +MarkDistributed(blockHash hash.Hash) error
        +MarkFailed(blockHash hash.Hash, reason string) error
        +GetDistributedBlocksSince(since int64) ([]hash.Hash, error)
        +GetBlockMetadata(blockHash hash.Hash) (*BlockMetadata, error)
    }

    %% CAS: Content Addressable Storage - the main management layer.
    %% Provides high-level API for content operations, coordinating
    %% encryption, WAL buffering, block storage, and access control.
    class CAS {
        %% Content operations (high-level API)
        +StoreContent(content []byte, parentHash hash.Hash) (Vertex, error)
        +GetContent(vertexHash hash.Hash) ([]byte, error)
        +DeleteContent(vertexHash hash.Hash) error
        
        %% Vertex operations
        +GetVertex(hash.Hash) (Vertex, error)
        +ListChildren(parentHash hash.Hash) ([]Vertex, error)
        
        %% Internal dependencies
        -DistributedWAL wal
        -BlockStore blockStore
        -EncryptionService encryptor
    }

    %% BlockStore: Low-level Block and BlockSlice persistence.
    %% Handles physical storage and retrieval of blocks and their shards.
    class BlockStore {
        +StoreBlock(Block) error
        +GetBlock(hash.Hash) (Block, error)
        +DeleteBlock(hash.Hash) error
        +StoreBlockSlice(BlockSlice) error
        +GetBlockSlice(hash.Hash) (BlockSlice, error)
        +ListBlockSlices(blockHash hash.Hash) ([]BlockSlice, error)

        %% Region-based retrieval for efficient partial reads
        +GetSealedChunkByRegion(blockHash hash.Hash, region ChunkRegion) (SealedChunk, error)
        +GetVertexByRegion(blockHash hash.Hash, region VertexRegion) (Vertex, error)
    }

    %% EncryptionService: Handles encryption/decryption of chunks.
    %% Manages the Chunk <-> SealedChunk transformation.
    class EncryptionService {
        +SealChunk(Chunk, pubKeys [][]byte) (SealedChunk, []KeyEntry, error)
        +UnsealChunk(SealedChunk, KeyEntry, privKey []byte) (Chunk, error)
        +GenerateKeyEntry(chunkHash hash.Hash, pubKey []byte, aesKey []byte) (KeyEntry, error)
    }

    class DeletionWAL {
        +LogDeletion(hash.Hash) error
        +ProcessDeletions() error
    }

    class DistributedIndex 

    class HashToNode {
        +GetNodeForHash(hash.Hash) Node
    }

    class KeyToHashAndNode {
        +GetHashAndNodeForKey(string) (hash.Hash, Node, error)
    }

    class DataReBalancer {
        +BalanceData()
    }

    class ReplicationMonitoring {
        +MonitorReplications()
    }

    class SyncIndexTree {
        +Sync()
    }

    class BackupManager {
        +BackupData()
    }

    Cluster "1" *-- "*" Node : contains
    Node "1" o-- "*" ClusterController : listensOn
    ClusterController "1" *-- "1" Carrier : communicatesVia
    ClusterController "1" *-- "*" HandlerRegistration : registers
    ClusterController ..> MessageHandler : invokes
    ClusterController ..> Response : returns
    ClusterController ..> AccessDecision : produces
    HandlerRegistration "1" o-- "*" TrustScope : allowsScopes
    HandlerRegistration "1" o-- "1" MessageHandler : executes
    Node "1" o-- "1" CAS : manages content
    Node "1" *-- "1" BlockStore : persists blocks
    DataRouter "1" *-- "1" ClusterController : interacts with
    CAS "1" *-- "1" DataRouter : coordinates distribution via
    DataRouter "1" o-- "1" BlockDistributionTracker : initiates distribution via
    ClusterController "1" o-- "1" BlockDistributionTracker : queries for offline node sync
    BlockDistributionTracker "1" *-- "*" Block : tracks distribution for
    CAS "1" *-- "1" BlockStore : stores blocks via
    CAS "1" *-- "1" EncryptionService : encrypts/decrypts via
    CAS "1" *-- "1" DistributedWAL : buffers writes via
    ClusterController "1" *-- "1" DistributedIndex : LooksUps
    DistributedIndex "1" *-- "1" HashToNode : used for mapping
    DistributedIndex "1" *-- "1" KeyToHashAndNode : lookups for keys
    ClusterController "1" *-- "1" DataReBalancer : manages
    DataReBalancer "1" *-- "1" ReplicationMonitoring : utilizes
    ClusterController "1" *-- "1" ClusterMonitor : monitors
    Node "1" *-- "1" BackupManager : manages backups
    ClusterMonitor "1" *-- "1" NodeAvailabilityTracker : utilizes
    ClusterMonitor "1" *-- "1" ClusterLog : aggregates
    ClusterMonitor "1" *-- "1" DataState : tracks
    ClusterMonitor "1" o-- "*" NodeStats : collects
    DataReBalancer "1" *-- "1" SyncIndexTree : utilizes
    Node "1" *-- "1" DeletionWAL : logs deletions

    %% Nodes emit logs into the cluster-wide log.
    Node ..> ClusterLog : emits logs to

    %% DataState maps data to nodes with status.
    %% Tracks objects from Logical_Hierarchy + Block_Storage_Modern:
    %% Vertex, SealedChunk, Block, BlockSlice, KeyEntry.
    DataState "1" o-- "*" Node : maps data to nodes

    %% NodeStats are associated with nodes.
    Node "1" o-- "*" NodeStats : reports

    %% (Diagram simplification) LogEntry/LogLevel are internal to ClusterLog.


    namespace IndexModel {
        class Index {
            -LocalIndexStore store
        }
        class parentChildIndex {
            - map<hash.Hash, []hash.Hash> ParentToChildren
            - map<hash.Hash, hash.Hash> ChildToParent
        }
        class VersionIndex {
            - map<hash.Hash, []hash.Hash> VersionVectorHeads
        }
        class KeyToHashIndex {
            - map<string, hash.Hash> KeyToHash
        }
    }

    Node "1" o-- "1" Index : indexes relations and metadata
    Index "1" o-- "1" parentChildIndex : manages
    Index "1" o-- "1" VersionIndex : manages

    namespace Logical_Hierarchy {
        %% Vertex (formerly Blob): The logical node in the DAG.
        %% stored UNENCRYPTED in the VertexSection of the Block.
        class Vertex {
            +hash.Hash Hash
            +hash.Hash Parent
            +int64 Created
            -[]hash.Hash ChunkHashes
            +GetContent() []byte
        }

        %% Chunk: The cleartext content.
        %% Only exists temporarily in memory during processing/decryption.
        class Chunk {
            +hash.Hash Hash
            +int Size
            -[]byte content
            +GetContent() []byte
        }
    }

    namespace Block_Storage_Modern {
        %% DistributedWAL: Intake buffer.
        %% Aggregates items until Block size (e.g., 16MB) is reached.
        class DistributedWAL {
            +AppendChunk(SealedChunk)
            +AppendVertex(Vertex)
            +SealBlock() Block
            -[]SealedChunk chunkBuffer
            -[]Vertex vertexBuffer
        }

        %% SealedChunk: The encrypted payload.
        %% Now includes SealedHash for integrity checking of the encrypted data 
        %% without needing decryption keys.
        class SealedChunk {
            +hash.Hash ChunkHash
            +hash.Hash SealedHash
            +[]byte EncryptedContent
            +[]byte Nonce
            +int OriginalSize
        }

        %% KeyEntry: The Access Control unit.
        %% Explicitly links the User (PubKey) to the Content (ChunkHash).
        class KeyEntry {
            +hash.Hash ChunkHash
            +hash.Hash PubKeyHash
            +[]byte EncapsulatedAESKey
        }

        %% The Indices that allow for byte range like requests
        %% This prevents that we need to decode the whole Block to get a single Chunk/Vertex.
        class ChunkRegion {
            +hash.Hash ChunkHash
            +uint32 Offset
            +uint32 Length
        }
        class VertexRegion {
            +hash.Hash VertexHash
            +uint32 Offset
            +uint32 Length
        }

        %% BlockHeader: Global block settings.
        class BlockHeader {
            +uint8 Version
            +int64 Created
            +uint8 RSDataSlices
            +uint8 RSParitySlices
            +uint32 ChunkCount
            +uint32 VertexCount
            +uint32 TotalSize
        }

        %% Block: The Central Archive.
        class Block {
            +hash.Hash Hash
            +BlockHeader Header
            +[]byte DataSection
            +[]byte VertexSection
            +map~Hash, []KeyEntry~ KeyRegistry
        }

        %% BlockSlice: The physical shard.
        class BlockSlice {
            +hash.Hash Hash
            +hash.Hash BlockHash
            +uint8 RSSliceIndex
            +uint8 RSDataSlices
            +uint8 RSParitySlices
            +[]byte Payload
        }
    }

    %% ==========================================
    %% 1. LOGICAL RELATIONSHIPS
    %% ==========================================
    Vertex "1" ..> "*" Chunk : references (via ChunkHash)

    %% ==========================================
    %% 2. PIPELINE & TRANSFORMATION
    %% ==========================================
    Chunk ..> SealedChunk : encrypts to
    SealedChunk --* DistributedWAL : buffered in
    Vertex --* DistributedWAL : buffered in
    DistributedWAL ..> Block : produces (SealBlock)

    %% ==========================================
    %% 3. BLOCK COMPOSITION
    %% ==========================================
    %% Structure
    Block "1" *-- "1" BlockHeader : has
    Block "1" *-- "*" ChunkRegion : indexes
    Block "1" *-- "*" VertexRegion : indexes
    
    %% Payloads
    Block "1" *-- "*" SealedChunk : contains (in DataSection)
    Block "1" *-- "*" Vertex : contains (in VertexSection)
    
    %% Registries
    Block "1" *-- "*" KeyEntry : registry (Access Control)

    %% ==========================================
    %% 4. PHYSICAL STORAGE
    %% ==========================================
    BlockSlice ..> Block : shards/reconstructs
    BlockStore "1" o-- "*" Block : persists
    BlockStore "1" o-- "*" BlockSlice : persists
    
    %% ==========================================
    %% 5. FUNCTIONAL LINKS
    %% ==========================================
    ChunkRegion ..> SealedChunk : locates bytes of
    VertexRegion ..> Vertex : locates bytes of
    KeyEntry ..> SealedChunk : unlocks (via ChunkHash)
    EncryptionService ..> SealedChunk : produces
    EncryptionService ..> KeyEntry : produces